{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfbc8d5f-7865-461b-963d-38ef3ca698ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddf65835-008f-4989-aa14-af7db38de430",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset/Images/Train/Banh gio/110.jpg\n",
      "./dataset/Images/Train/Banh gio/468.jpg\n",
      "./dataset/Images/Train/Banh gio/384.jpg\n",
      "./dataset/Images/Train/Canh chua/101.jpg\n",
      "./dataset/Images/Train/Canh chua/499.jpg\n",
      "./dataset/Images/Train/Canh chua/687.jpg\n",
      "./dataset/Images/Train/Canh chua/80.jpg\n",
      "./dataset/Images/Train/Canh chua/92.jpg\n",
      "./dataset/Images/Train/Canh chua/147.jpg\n",
      "./dataset/Images/Train/Banh cuon/88.jpg\n",
      "./dataset/Images/Train/Banh cuon/26.jpg\n",
      "./dataset/Images/Train/Banh cuon/781.jpg\n",
      "./dataset/Images/Train/Banh cuon/350.jpg\n",
      "./dataset/Images/Train/Nem chua/98.jpg\n",
      "./dataset/Images/Train/Nem chua/314.jpg\n",
      "./dataset/Images/Train/Nem chua/112.jpg\n",
      "./dataset/Images/Train/Nem chua/267.jpg\n",
      "./dataset/Images/Train/Nem chua/281.jpg\n",
      "./dataset/Images/Train/Nem chua/291.jpg\n",
      "./dataset/Images/Train/Nem chua/424.jpg\n",
      "./dataset/Images/Train/Nem chua/422.jpg\n",
      "./dataset/Images/Train/Nem chua/423.jpg\n",
      "./dataset/Images/Train/Banh chung/11.jpg\n",
      "./dataset/Images/Train/Banh chung/5.jpg\n",
      "./dataset/Images/Train/Banh chung/43.jpg\n",
      "./dataset/Images/Train/Chao long/86.jpg\n",
      "./dataset/Images/Train/Banh beo/268.jpg\n",
      "./dataset/Images/Train/Banh beo/356.jpg\n",
      "./dataset/Images/Train/Banh trang nuong/144.jpg\n",
      "./dataset/Images/Train/Banh canh/219.jpg\n",
      "./dataset/Images/Train/Banh khot/24.jpg\n",
      "./dataset/Images/Train/Banh khot/84.jpg\n",
      "./dataset/Images/Train/Banh tet/381.jpg\n",
      "./dataset/Images/Train/Banh tet/191.jpg\n",
      "./dataset/Images/Train/Cao lau/212.jpg\n",
      "./dataset/Images/Train/Cao lau/243.jpg\n",
      "./dataset/Images/Train/Goi cuon/363.jpg\n",
      "./dataset/Images/Train/Goi cuon/210.jpg\n",
      "./dataset/Images/Train/Goi cuon/446.jpg\n",
      "./dataset/Images/Train/Goi cuon/545.jpg\n",
      "./dataset/Images/Train/Goi cuon/155.jpg\n",
      "./dataset/Images/Train/Banh duc/327.jpg\n",
      "./dataset/Images/Train/Ca kho to/89.jpg\n",
      "./dataset/Images/Train/Ca kho to/301.jpg\n",
      "./dataset/Images/Train/Ca kho to/37.jpg\n",
      "./dataset/Images/Train/Ca kho to/544.jpg\n",
      "./dataset/Images/Train/Ca kho to/86.jpg\n",
      "./dataset/Images/Train/Bun bo Hue/1530.jpg\n",
      "./dataset/Images/Train/Bun bo Hue/502.jpg\n",
      "./dataset/Images/Train/Bun bo Hue/643.jpg\n",
      "./dataset/Images/Train/Bun bo Hue/427.jpg\n",
      "./dataset/Images/Train/Bun mam/199.jpg\n",
      "./dataset/Images/Train/Bun mam/20.jpg\n",
      "./dataset/Images/Train/Bun mam/252.jpg\n",
      "./dataset/Images/Train/Mi quang/117.jpg\n",
      "./dataset/Images/Train/Mi quang/69.jpg\n",
      "./dataset/Images/Train/Banh mi/623.jpg\n",
      "./dataset/Images/Train/Bun rieu/758.jpg\n",
      "./dataset/Images/Train/Bun rieu/89.jpg\n",
      "./dataset/Images/Train/Bun rieu/106.jpg\n",
      "./dataset/Images/Train/Bun rieu/647.jpg\n",
      "./dataset/Images/Train/Bun rieu/325.jpg\n",
      "./dataset/Images/Train/Bun dau mam tom/351.jpg\n",
      "./dataset/Images/Train/Banh bot loc/165.jpg\n",
      "./dataset/Images/Train/Banh bot loc/416.jpg\n",
      "./dataset/Images/Train/Banh bot loc/98.jpg\n",
      "./dataset/Images/Train/Banh bot loc/714.jpg\n",
      "./dataset/Images/Train/Banh xeo/820.jpg\n",
      "./dataset/Images/Train/Banh xeo/428.jpg\n",
      "./dataset/Images/Train/Banh xeo/307.jpg\n",
      "./dataset/Images/Train/Banh xeo/118.jpg\n",
      "./dataset/Images/Train/Banh xeo/196.jpg\n",
      "./dataset/Images/Train/Banh can/359.jpg\n",
      "./dataset/Images/Train/Bun thit nuong/260.jpg\n",
      "./dataset/Images/Train/Bun thit nuong/488.jpg\n",
      "./dataset/Images/Train/Bun thit nuong/109.jpg\n",
      "./dataset/Images/Train/Bun thit nuong/326.jpg\n",
      "./dataset/Images/Train/Bun thit nuong/209.jpg\n",
      "./dataset/Images/Train/Com tam/258.jpg\n",
      "./dataset/Images/Train/Com tam/481.jpg\n",
      "./dataset/Images/Train/Com tam/497.jpg\n",
      "./dataset/Images/Train/Com tam/569.jpg\n",
      "./dataset/Images/Train/Banh pia/310.jpg\n",
      "./dataset/Images/Train/Banh pia/123.jpg\n",
      "./dataset/Images/Train/Banh pia/269.jpg\n",
      "./dataset/Images/Train/Banh pia/27.jpg\n",
      "./dataset/Images/Train/Banh pia/192.jpg\n",
      "./dataset/Images/Train/Banh pia/91.jpg\n",
      "./dataset/Images/Train/Pho/613.jpg\n",
      "./dataset/Images/Train/Pho/273.jpg\n",
      "./dataset/Images/Train/Pho/125.jpg\n",
      "./dataset/Images/Test/Banh gio/251.jpg\n",
      "./dataset/Images/Test/Canh chua/135.jpg\n",
      "./dataset/Images/Test/Banh cuon/677.jpg\n",
      "./dataset/Images/Test/Banh cuon/45.jpg\n",
      "./dataset/Images/Test/Banh cuon/634.jpg\n",
      "./dataset/Images/Test/Banh tet/140.jpg\n",
      "./dataset/Images/Test/Goi cuon/224.jpg\n",
      "./dataset/Images/Test/Banh duc/487.jpg\n",
      "./dataset/Images/Test/Hu tieu/269.jpg\n",
      "./dataset/Images/Test/Bun bo Hue/785.jpg\n",
      "./dataset/Images/Test/Mi quang/870.jpg\n",
      "./dataset/Images/Test/Banh mi/860.jpg\n",
      "./dataset/Images/Test/Bun dau mam tom/175.jpg\n",
      "./dataset/Images/Test/Banh bot loc/405.jpg\n",
      "./dataset/Images/Test/Banh xeo/403.jpg\n",
      "./dataset/Images/Test/Banh xeo/318.jpg\n",
      "./dataset/Images/Test/Bun thit nuong/452.jpg\n",
      "./dataset/Images/Test/Banh pia/100.jpg\n"
     ]
    }
   ],
   "source": [
    "# training set과 test set의 모든 이미지 파일에 대해서,\n",
    "# jpg image header가 포함되지 않은 (jpg의 파일 구조에 어긋나는) 파일들을 삭제해줍니다.\n",
    "\n",
    "data_path = './dataset/Images/'\n",
    "train_path = data_path + 'Train/'\n",
    "val_path = data_path + 'Validate/'\n",
    "test_path = data_path + 'Test/'\n",
    "\n",
    "for path in [train_path, test_path]:\n",
    "    classes = os.listdir(path)\n",
    "\n",
    "    for food in classes:\n",
    "        food_path = os.path.join(path, food)\n",
    "        images = os.listdir(food_path)\n",
    "        \n",
    "        for image in images:\n",
    "            with open(os.path.join(food_path, image), 'rb') as f:\n",
    "                bytes = f.read()\n",
    "            if bytes[:3] != b'\\xff\\xd8\\xff':\n",
    "                print(os.path.join(food_path, image))\n",
    "                os.remove(os.path.join(food_path, image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f3e6b79-7cac-4783-a752-ae5bc51eccab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Banh gio',\n",
       " 'Canh chua',\n",
       " 'Banh cuon',\n",
       " 'Nem chua',\n",
       " 'Banh chung',\n",
       " 'Chao long',\n",
       " 'Banh beo',\n",
       " 'Banh trang nuong',\n",
       " 'Banh canh',\n",
       " 'Banh khot']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10개만 사용\n",
    "cls = [x for x in os.listdir(train_path) if os.path.isdir(os.path.join(train_path, x))][:10]\n",
    "cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a3cc7a5-2f9b-422c-be87-72c1ebb4d071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data의 개수: 5540\n",
      "validate data의 개수: 798\n",
      "test data의 개수: 1591\n"
     ]
    }
   ],
   "source": [
    "train_length = 0\n",
    "test_length = 0\n",
    "val_length = 0\n",
    "\n",
    "for food in cls:\n",
    "    train_food_path = os.path.join(train_path, food)\n",
    "    test_food_path = os.path.join(test_path, food)\n",
    "    val_food_path = os.path.join(val_path, food)\n",
    "    \n",
    "    images_train = os.listdir(train_food_path)\n",
    "    images_test = os.listdir(test_food_path)\n",
    "    images_val = os.listdir(val_food_path)\n",
    "    \n",
    "    train_length += len(images_train)\n",
    "    test_length += len(images_test)\n",
    "    val_length += len(images_val)\n",
    "\n",
    "print('training data의 개수: '+str(train_length))\n",
    "print('validate data의 개수: '+str(val_length))\n",
    "print('test data의 개수: '+str(test_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9f9a8642-79ce-4e1e-a696-b5debb163616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def process_path(file_path, class_names, img_shape=(224, 224)):\n",
    "    '''\n",
    "    file_path로부터 class label을 만들고, 이미지를 읽는 함수\n",
    "    '''\n",
    "    label = tf.strings.split(file_path, os.path.sep)\n",
    "    label = label[-2] == class_names\n",
    "\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    img = tf.image.resize(img, img_shape)\n",
    "    return img, label\n",
    "\n",
    "def prepare_for_training(ds, batch_size=32, cache=True, shuffle_buffer_size=1000):\n",
    "    '''\n",
    "    TensorFlow Data API를 이용해 data batch를 만드는 함수\n",
    "    '''\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "        else:\n",
    "            ds = ds.cache()\n",
    "\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return ds\n",
    "\n",
    "def load_data(data_path, cls, img_shape, batch_size=32, is_train=True):\n",
    "    '''\n",
    "    데이터를 만들기 위해 필요한 함수들을 호출하고 데이터를 리턴해주는 함수\n",
    "    '''\n",
    "    #class_names = [cls for cls in os.listdir(data_path) if cls != '.DS_Store']\n",
    "    class_names = cls\n",
    "    data_path = pathlib.Path(data_path)\n",
    "\n",
    "    list_ds = tf.data.Dataset.list_files(str(data_path/'*/*'))\n",
    "    labeled_ds = list_ds.map(lambda x: process_path(x, class_names, img_shape=(224, 224)))\n",
    "    ds = prepare_for_training(labeled_ds, batch_size=batch_size)\n",
    "    DATASET_SIZE = tf.data.experimental.cardinality(list_ds).numpy()\n",
    "\n",
    "    return ds, DATASET_SIZE\n",
    "def show_batch(image_batch, label_batch, class_names):\n",
    "    size = len(image_batch)\n",
    "    sub_size = int(size ** 0.5) + 1\n",
    "\n",
    "    plt.figure(figsize=(10, 10), dpi=80)\n",
    "    for n in range(size):\n",
    "        plt.subplot(sub_size, sub_size, n+1)\n",
    "        plt.subplots_adjust(left=0.125, bottom=0.1, right=0.9, top=0.9, wspace=0.2, hspace=0.5)\n",
    "        plt.title(class_names[np.array(label_batch[n])==True][0].title())\n",
    "        plt.imshow(image_batch[n])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f89bde59-6397-4d67-b40f-039a2acbcceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_ds, TRAIN_SIZE = load_data(data_path=train_path, cls= cls, img_shape=(224, 224), batch_size=batch_size)\n",
    "val_ds, VAL_SIZE = load_data(data_path=val_path, cls=cls,img_shape=(224, 224), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4df8f5c9-915e-4171-8d35-7d1b187638da",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_steps_per_epoch = lambda x: int(math.ceil(1. * x / batch_size))\n",
    "steps_per_epoch = compute_steps_per_epoch(TRAIN_SIZE)\n",
    "val_steps = compute_steps_per_epoch(VAL_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c1ae4c-06df-47f5-9eb7-01d6a59d7d6d",
   "metadata": {},
   "source": [
    "# 모델 : EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1cc210f4-f21e-4efd-8de2-903fe15102e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetb0 (Functional  (None, None, None, 1280   4049571   \n",
      " )                           )                                   \n",
      "                                                                 \n",
      " sequential_6 (Sequential)   (None, 1280)              5120      \n",
      "                                                                 \n",
      " pred (Dense)                multiple                  12810     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4067501 (15.52 MB)\n",
      "Trainable params: 15370 (60.04 KB)\n",
      "Non-trainable params: 4052131 (15.46 MB)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "\n",
    "class myModel(tf.keras.Model):\n",
    "    '''\n",
    "    EfficientNetB0을 백본으로 사용하는 모델을 구성합니다.\n",
    "    Classification 문제로 접근할 것이기 때문에 맨 마지막 Dense 레이어에 \n",
    "    우리가 원하는 클래스 갯수 만큼을 지정해주어야 합니다.\n",
    "    '''\n",
    "    def __init__(self, num_classes=5, freeze=False):\n",
    "        super(myModel, self).__init__()\n",
    "        self.base_model = EfficientNetB0(include_top=False, weights='imagenet')\n",
    "        if freeze:\n",
    "            self.base_model.trainable = False\n",
    "        self.top = tf.keras.Sequential([tf.keras.layers.GlobalAveragePooling2D(name=\"avg_pool\"),\n",
    "                                       tf.keras.layers.BatchNormalization(),\n",
    "                                       tf.keras.layers.Dropout(0.5, name=\"top_dropout\")])\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes, activation=\"softmax\", name=\"pred\")\n",
    "    def call(self, inputs, training=True):\n",
    "        x = self.base_model(inputs)\n",
    "        x = self.top(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = myModel(num_classes=10, freeze=True)\n",
    "    model.build(input_shape=(None, 224, 224, 3))\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4698be03-6492-4255-acbd-bb940778c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Progbar\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, epochs, batch, loss_fn, optimizer):\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.batch = batch\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def compute_acc(self, y_pred, y):\n",
    "        correct = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        return accuracy\n",
    "\n",
    "    @tf.function\n",
    "    def train_on_batch(self, x_batch_train, y_batch_train):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)    # 모델이 예측한 결과\n",
    "            train_loss = self.loss_fn(y_batch_train, logits)     # 모델이 예측한 결과와 GT를 이용한 loss 계산\n",
    "\n",
    "        grads = tape.gradient(train_loss, model.trainable_weights)  # gradient 계산\n",
    "        self.optimizer.apply_gradients(zip(grads, model.trainable_weights))  # Otimizer에게 처리된 그라데이션 적용을 요청\n",
    "\n",
    "        return train_loss, logits\n",
    "\n",
    "    def train(self, train_dataset, acc_metric, steps_per_epoch, val_dataset, val_step):\n",
    "        metrics_names = ['train_loss', 'train_acc', 'val_loss']\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            print(\"\\nEpoch {}/{}\".format(epoch+1, self.epochs))\n",
    "\n",
    "            train_dataset = train_dataset.shuffle(100)\n",
    "            val_dataset = val_dataset.shuffle(100)\n",
    "\n",
    "            train_dataset = train_dataset.take(steps_per_epoch)\n",
    "            val_dataset = val_dataset.take(val_step)\n",
    "\n",
    "            progBar = Progbar(steps_per_epoch * self.batch, stateful_metrics=metrics_names)\n",
    "\n",
    "            train_loss, val_loss = 100, 100\n",
    "\n",
    "            # 데이터 집합의 배치에 대해 반복합니다\n",
    "            for step_train, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "                train_loss, logits = self.train_on_batch(x_batch_train, y_batch_train)\n",
    "\n",
    "                # train metric(mean, auc, accuracy 등) 업데이트\n",
    "                acc_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "                train_acc = self.compute_acc(logits, y_batch_train)\n",
    "                values = [('train_loss', train_loss), ('train_acc', train_acc)]\n",
    "                # print('{}'.format((step_train + 1) * self.batch))\n",
    "                progBar.update((step_train + 1) * self.batch, values=values)\n",
    "\n",
    "            for step, (x_batch_val, y_batch_val) in enumerate(val_dataset):\n",
    "                logits = model(x_batch_val, training=False)\n",
    "                val_loss = self.loss_fn(y_batch_val, logits)\n",
    "                val_acc = self.compute_acc(logits, y_batch_val)\n",
    "                values = [('train_loss', train_loss), ('train_acc', train_acc), ('val_loss', val_loss), ('val_acc', val_acc)]\n",
    "            progBar.update((step_train + 1) * self.batch, values=values, finalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "15466b5a-37d8-4734-9428-03db7d501d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 14:22:51.293415: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 337/1094 [========>.....................] - ETA: 2:58 - train_loss: 1.1022 - train_acc: 0.1875 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 9 extraneous bytes before marker 0xe2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 759/1094 [===================>..........] - ETA: 1:18 - train_loss: 0.9016 - train_acc: 0.0625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: 229 extraneous bytes before marker 0xd9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1094/1094 [==============================] - 272s 232ms/step - train_loss: 1.0070 - train_acc: 0.1250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 14:27:13.957608: W tensorflow/core/lib/png/png_io.cc:88] PNG warning: iCCP: known incorrect sRGB profile\n",
      "2024-04-08 14:27:14.075667: W tensorflow/core/lib/png/png_io.cc:88] PNG warning: iCCP: known incorrect sRGB profile\n",
      "2024-04-08 14:27:16.169183: W tensorflow/core/lib/png/png_io.cc:88] PNG warning: iCCP: known incorrect sRGB profile\n",
      "2024-04-08 14:27:16.169198: W tensorflow/core/lib/png/png_io.cc:88] PNG warning: iCCP: cHRM chunk does not match sRGB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1094/1094 [==============================] - 311s 267ms/step - train_loss: 1.0070 - train_acc: 0.1250 - val_loss: 1.0860 - val_acc: 0.0000e+00\n",
      "\n",
      "Epoch 2/3\n",
      "1094/1094 [==============================] - 254s 227ms/step - train_loss: 1.0654 - train_acc: 0.1875\n",
      "1094/1094 [==============================] - 285s 256ms/step - train_loss: 1.0654 - train_acc: 0.1875 - val_loss: 1.2182 - val_acc: 0.0625\n",
      "\n",
      "Epoch 3/3\n",
      "1094/1094 [==============================] - 266s 234ms/step - train_loss: 1.0000 - train_acc: 0.2500\n",
      "1094/1094 [==============================] - 298s 264ms/step - train_loss: 1.0000 - train_acc: 0.2500 - val_loss: 1.0587 - val_acc: 0.0625\n"
     ]
    }
   ],
   "source": [
    "loss_function = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.3)\n",
    "acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0001)\n",
    "\n",
    "model = myModel(num_classes=10)\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "manager = tf.train.CheckpointManager(checkpoint, directory=\".\", max_to_keep=5)\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  epochs=3,\n",
    "                  batch=1,\n",
    "                  loss_fn=loss_function,\n",
    "                  optimizer=optimizer,)\n",
    "\n",
    "trainer.train(train_dataset=train_ds,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            val_step=val_steps,\n",
    "            val_dataset=val_ds,\n",
    "            acc_metric=acc_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f70befc8-4b4e-4da7-ac3c-53b514d337e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/16\n",
      "13/16\n",
      "11/16\n",
      "8/16\n",
      "14/16\n",
      "10/16\n",
      "12/16\n",
      "10/16\n",
      "14/16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 14:40:40.327556: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/16\n"
     ]
    }
   ],
   "source": [
    "num_classes = 1 \n",
    "epoch = 1 \n",
    "batch_size = 16\n",
    "img_size = 224\n",
    "\n",
    "checkpoint_path = './checkpoints/'\n",
    "\n",
    "model = myModel(num_classes=num_classes)\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0001)\n",
    "\n",
    "test, TEST_SIZE = load_data(data_path=test_path, cls=cls, img_shape=(img_size, img_size), batch_size=batch_size, is_train=False)\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_path))\n",
    "\n",
    "for step_train, (x_batch_train, y_batch_train) in enumerate(test.take(10)):\n",
    "    #print(model(x_batch_train))\n",
    "    prediction = model(x_batch_train)\n",
    "    #print(tf.argmax(y_batch_train, axis=1))\n",
    "    #print(tf.argmax(prediction, axis=1))\n",
    "    #print(tf.equal(tf.argmax(y_batch_train, axis=1), tf.argmax(prediction, axis=1)))\n",
    "    print(\"{}/{}\".format(np.array(tf.equal(tf.argmax(y_batch_train, axis=1), tf.argmax(prediction, axis=1))).sum(), tf.argmax(y_batch_train, axis=1).shape[0]))\n",
    "    #print(\"Prediction: {}\".format(tf.argmax(prediction, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92371db8-2080-4b21-9656-6106cb910a55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
