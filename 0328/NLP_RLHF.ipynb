{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01ad29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d04afe49e146cc918d0566327d0eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f3437afb33040dba423af4c46cd639d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4b68dd0e744708afb6214198babe8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = 'skt/kogpt2-base-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "846688c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt2': 1024,\n",
       " 'gpt2-medium': 1024,\n",
       " 'gpt2-large': 1024,\n",
       " 'gpt2-xl': 1024,\n",
       " 'distilgpt2': 1024}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec38aeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_txt = \"바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00a8508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(input_txt).tokens()\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02eaaf36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>kogpt-2_tokens</th>\n",
       "      <td>▁바람</td>\n",
       "      <td>도</td>\n",
       "      <td>▁없는</td>\n",
       "      <td>▁공</td>\n",
       "      <td>중에</td>\n",
       "      <td>▁수직</td>\n",
       "      <td>의</td>\n",
       "      <td>▁파</td>\n",
       "      <td>문을</td>\n",
       "      <td>▁내</td>\n",
       "      <td>이며</td>\n",
       "      <td>▁고요</td>\n",
       "      <td>히</td>\n",
       "      <td>▁떨어지는</td>\n",
       "      <td>▁오</td>\n",
       "      <td>동</td>\n",
       "      <td>잎</td>\n",
       "      <td>은</td>\n",
       "      <td>▁누구</td>\n",
       "      <td>의</td>\n",
       "      <td>▁발자</td>\n",
       "      <td>취</td>\n",
       "      <td>▁입</td>\n",
       "      <td>니까.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Input_IDs</th>\n",
       "      <td>31140</td>\n",
       "      <td>20780</td>\n",
       "      <td>30359</td>\n",
       "      <td>30016</td>\n",
       "      <td>31373</td>\n",
       "      <td>41427</td>\n",
       "      <td>25792</td>\n",
       "      <td>30163</td>\n",
       "      <td>31047</td>\n",
       "      <td>30024</td>\n",
       "      <td>31111</td>\n",
       "      <td>51068</td>\n",
       "      <td>29936</td>\n",
       "      <td>36152</td>\n",
       "      <td>30027</td>\n",
       "      <td>20801</td>\n",
       "      <td>25846</td>\n",
       "      <td>25768</td>\n",
       "      <td>31199</td>\n",
       "      <td>25792</td>\n",
       "      <td>44202</td>\n",
       "      <td>27472</td>\n",
       "      <td>30148</td>\n",
       "      <td>37708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0      1      2      3      4      5      6      7      8   \\\n",
       "kogpt-2_tokens    ▁바람      도    ▁없는     ▁공     중에    ▁수직      의     ▁파     문을   \n",
       "Input_IDs       31140  20780  30359  30016  31373  41427  25792  30163  31047   \n",
       "\n",
       "                   9      10     11     12     13     14     15     16     17  \\\n",
       "kogpt-2_tokens     ▁내     이며    ▁고요      히  ▁떨어지는     ▁오      동      잎      은   \n",
       "Input_IDs       30024  31111  51068  29936  36152  30027  20801  25846  25768   \n",
       "\n",
       "                   18     19     20     21     22     23  \n",
       "kogpt-2_tokens    ▁누구      의    ▁발자      취     ▁입    니까.  \n",
       "Input_IDs       31199  25792  44202  27472  30148  37708  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = 40\n",
    "pd.options.display.max_rows = 60\n",
    "df = pd.DataFrame([tokens, input_ids[0]], index=[\"kogpt-2_tokens\", \"Input_IDs\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "820b622d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까. 누가 그 길을 걸었습니까. 누가 이 길을 걸어 왔습니까. 나는 지금 누구를 기다리는 것입니까. 이 길은 내 발길만이 아니라 나의 발자국도 함께 남긴 길인 것입니다. -김형효 산문집<unk>누구를 기다리는가<unk>중에서- \n",
      " <unk> 이 시는 한 여인이 사랑하는 사람을 기다리는 심경을 담담히 그려낸 시이다. 사랑하는 사람에게 다가가고 싶으면서도 다가가지 못하는 심경도 절절하게 담겨 있다. 사랑할 수 없는 것이 사랑이라는 것은 누구나의 경험이다. 이 시에서 여인은\n"
     ]
    }
   ],
   "source": [
    "max_length=128\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=7, no_repeat_ngram_size=2,\n",
    "                             do_sample=True, temperature=2.0, top_k=50)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6f61afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.</d>\n"
     ]
    }
   ],
   "source": [
    "max_length=128\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=7, no_repeat_ngram_size=2,\n",
    "                             do_sample=True, top_p=0.80)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a64751",
   "metadata": {},
   "source": [
    "# SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2e740c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '불고기용 고기 한우에요?',\n",
       "  'completion': \"'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\",\n",
       "  'tokens': 193},\n",
       " {'prompt': '쓰던 앱이 유료로 전환됐어',\n",
       "  'completion': \"'어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 유료 전환된 앱은 기존 무료 앱에서 추가적인 기능이나 서비스를 제공하는 경우가 많습니다. 이 경우, 유료 전환 이전 가격이 매우 저렴하거나 무료인 경우가 많으므로, 앱을 계속 사용하려면 추가적인 비용을 지불해야 할 수 있습니다. 하지만, 유료 전환 이후에는 기존 사용자 또는 새로운 사용자 모두가 추가 기능과 높은 품질의 서비스를 이용할 수 있게 됩니다. 따라서, 앱 개발자는 유료 모델로 전환함으로써 앱의 수익을 증가시키고 더 나은 서비스를 제공할 수 있습니다.\",\n",
       "  'tokens': 288},\n",
       " {'prompt': '여친이랑 다툼',\n",
       "  'completion': \"'저는 인공지능이기 때문에 여친과의 다툼에 대해서는 조언을 드릴 수는 없습니다. 하지만 다투게 되면 상대방의 말을 경청하는 태도와 서로의 감정을 존중하는 것이 중요합니다. 논리적이고 차분한 대화를 통해 문제를 해결해보세요. 그리고 서로를 이해하고 서로의 의견을 수용하는 것이 중요합니다.\",\n",
       "  'tokens': 153}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "data_path_1_SFT = '../KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl' \n",
    "with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc516fe",
   "metadata": {},
   "source": [
    "# RM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9b9c226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?',\n",
       "  'completion_0': 'Allow me to answer your question. I know that you are curious about me.',\n",
       "  'completion_1': '번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.',\n",
       "  'completion_2': '라이언에게 말했다.',\n",
       "  'ranking': [2, 1, 0]},\n",
       " {'prompt': '개포주공아파트는 몇 단지로 이루어져 있나?',\n",
       "  'completion_0': '개포주공아파트는 다섯 단지로 이루어져 있습니다.',\n",
       "  'completion_1': '이날 목송에서 구글상위노',\n",
       "  'completion_2': '개포주공아파트는 총 27개 단지로 이루어져 있습니다.',\n",
       "  'ranking': [2, 0, 1]},\n",
       " {'prompt': '김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?',\n",
       "  'completion_0': 'The diameter of the Metallic domain is bigger than the Hyperonic domain.',\n",
       "  'completion_1': '이 질문은 조금 불분명합니다. 김영삼 대통령이 후보 시절에 어떤 발언을 했고, 누가 그 발언을 문제삼았는지에 따라 답이 다를 수 있습니다.\\\\n\\\\n만약 김영삼 대통령이 후보 시절에 지역표심을 겨냥한 발언을 했다는 가정하에, 그 발언을 문제삼은 후보가 누구였는지를 대답하자면, 그 답은 이화선 당시 민주당 대통령 후보가 될 것입니다. 1992년 총선 때, 김영삼 대선후보는 \"집값이 오른 노량진역 부근의 부동산 가격은 세월호 폭침 후 \\\\\\'강남 도시재생\\\\\\' 일환으로 상승했다\"는 발언을 했습니다. 하지만 이화선 후보는 이 발언을 \"전국적으로 경제적 발전이 이루어지지 않은 지방민의 마음을 멀리해지려는 무례한 발언\"이라고 비판하며 문제삼았습니다.\\\\n\\\\n하지만, 이 질문을 답변하는 데 있어서 보다 명확한 정보가 있으면 답변을 보완할 수 있습니다.',\n",
       "  'completion_2': '김영삼의 후보 시절에 지역표심을 겨냥한 발언은 대통령 당선 전까지 대한민국 정부가 추구하고 있는 민주주의 광범위하게 확립과 보수의 사상을 이어가는 데 있어 지역경제 발전과 공공서비스 신속 개선을 위해 합리적인 국가 정책에 따르는 방향성을 제시하고 있습니다.',\n",
       "  'ranking': [1, 2, 0]}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path_2_RM = '../KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl'\n",
    "with open(data_path_2_RM, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0f1980",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11348e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?'},\n",
       " {'prompt': '개포주공아파트는 몇 단지로 이루어져 있나?'},\n",
       " {'prompt': '김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path_3_PPO = '../KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl'\n",
    "with open(data_path_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08e77800",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8875d8e5",
   "metadata": {},
   "source": [
    "# SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b457710",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98d73089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4720e696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "799401b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "class SFT_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_output = 'completion'  # response\n",
    "\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "\n",
    "        PROMPT_DICT = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
    "\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            tmp = prompt_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36be79fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object): \n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81e1bf1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 12000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : tensor([  739,   378,   378,   378, 14659, 13394, 37091, 10651,   383, 25841,\n",
      "         8006, 14914,   375,  7673, 20479,  8091, 22311,  9036, 30902, 13675,\n",
      "          375,   378,   378,   378, 41951,   454,  9549, 20549,   383,  8142,\n",
      "         7192, 14914,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n",
      "output: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SFT_dataset(data_path_1_SFT='../KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl', tokenizer=tokenizer)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "print('input : %s'%train_dataset.input_ids[0])\n",
    "print('output: %s'%train_dataset.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bf2b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/aiffel/KoChatGPT/test\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    fp16 = True\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13ca7aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 27:34, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.011000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.865600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.819900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.280600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.310200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.304400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.840300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.885500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.887800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.531900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.543200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.563700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.297200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.329400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.321800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained('../KoChatGPT/output_1_SFT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c802f7",
   "metadata": {},
   "source": [
    "# 기본 모델과의 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994309b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_SFT(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "\n",
    "    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n",
    "\n",
    "    return output_reward\n",
    "\n",
    "input_text = '인공지능은 똥멍청이 입니다'\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8543d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_txt = \"바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fce77eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까. 그리고 그것이 어디인지 알 수 없어 정확한 답변을 제공할 수 없습니다. 만약 딥러닝 기반 대화 모델로 자연어 처리가 가능하다면 질문에 대한 충분한 답을 제공해 주시기 바랍니다.</s>\n"
     ]
    }
   ],
   "source": [
    "max_length=128\n",
    "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_beam = model.generate(input_ids, max_length=max_length, num_beams=7, no_repeat_ngram_size=2,\n",
    "                             do_sample=True, temperature=2.0, top_k=50)\n",
    "print(tokenizer.decode(output_beam[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "40ba6c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까. 누가 그 길을 걸었습니까. 누가 이 길을 걸어 왔습니까. 나는 지금 누구를 기다리는 것입니까. 이 길은 내 발길만이 아니라 나의 발자국도 함께 남긴 길인 것입니다. -김형효 산문집<unk>누구를 기다리는가<unk>중에서- <unk> 이 시는 한 여인이 사랑하는 사람을 기다리는 심경을 담담히 그려낸 시이다. 사랑하는 사람에게 다가가고 싶으면서도 다가가지 못하는 심경도 절절하게 담겨 있다. 사랑할 수 없는 것이 사랑이라는 것은 누구나의 경험이다. 이 시에서 여인은\n"
     ]
    }
   ],
   "source": [
    "#kogpt version\n",
    "kogpt_v = \"바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까. 누가 그 길을 걸었습니까. 누가 이 길을 걸어 왔습니까. 나는 지금 누구를 기다리는 것입니까. 이 길은 내 발길만이 아니라 나의 발자국도 함께 남긴 길인 것입니다. -김형효 산문집<unk>누구를 기다리는가<unk>중에서- <unk> 이 시는 한 여인이 사랑하는 사람을 기다리는 심경을 담담히 그려낸 시이다. 사랑하는 사람에게 다가가고 싶으면서도 다가가지 못하는 심경도 절절하게 담겨 있다. 사랑할 수 없는 것이 사랑이라는 것은 누구나의 경험이다. 이 시에서 여인은\"\n",
    "print(kogpt_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b2c3bd",
   "metadata": {},
   "source": [
    "주어진 문장에 대해 질문으로 인식하는 양상을 보인다. 따라서 kopgt같은 경우 그 다음의 단어를 예측하여 generation을 중점적으로 하는 방면, SFT 모델의 경우 질문에 대한 답을 줄수 없으면 줄 수 없다고 대답하는 양상을 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4345f42",
   "metadata": {},
   "source": [
    "# Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1fd6397",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ec63d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer import RewardModelTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "import loralib as lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60100259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTRM_custom(RewardModel):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[GPT2Config] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None) -> None:\n",
    "        if pretrained is not None:\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        elif config is not None:\n",
    "            model = GPT2Model(config)\n",
    "        else:\n",
    "            model = GPT2Model(GPT2Config())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c20f474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at skt/kogpt2-base-v2 were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "with NaiveStrategy().model_init_context():\n",
    "        model = GPTRM_custom(pretrained='skt/kogpt2-base-v2', lora_rank=0, tokenizer=tokenizer).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cbe5650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before data num: 10220\n",
      "after  data num: 30660\n",
      "data example: \n",
      "{'prompt': '애플은 리사를 어떻게 처리했어', 'chosen': '애플이 누구인지 명확히 알 수 없어서, 리사가 누구인지와 어떤 상황에서 처리되었는지에 대한 추가적인 정보가 필요합니다. 따라서, 보다 정확한 답변을 제공할 수 없습니다.', 'rejected': '애플은 리사를 위해 고객 서비스 부서에서 고객 다양한 컴퓨터 관련 문제에 대해 응답하는 데 필요한 모든 지원을 제공했습니다. 사용자가 하드웨어 문제를 경험할 때, 전문가들은 필요한 수리(수리, 추가 부품 제공, 소프트웨어 업그레이드 등)을 제공해 드릴 수 있습니다. 또한, 사용자가 사용 방법 문제나 기타 문제를 경험할 때, 대화 상대로 사용자를 지원할 수 있는 전문 고객 서비스 직원들이 사용자에게 상담하고 도움을 주는 데 도움이 될 수 있는 정보를 제공합니다. 또한, 인터넷에서 제공되는 정보를 통해 문제를 해결하거나 고객 서비스 웹 사이트를 통해 자신의 문제를 진단할 수 있도록 하는 등 다양한 방법으로 리사를 처리해 왔습니다.'}\n"
     ]
    }
   ],
   "source": [
    "with open('/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "\n",
    "\n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print('before data num: %d'%(len(list_data_dict)))\n",
    "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s'%total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e381c84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '유아인이 류승완 감독을 만나 영화 베테랑의 시나리오를 받았던 곳은?', 'chosen': '유아인이 류승완 감독을 만나 영화 베테랑의 시나리오를 받았던 곳은 류승완의 사무실입니다.', 'rejected': '대구 영화사옥'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(230319)\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7f7e357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1240.97it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1147.46it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = total_data_ranking2chosen[:1000] \n",
    "eval_data = total_data_ranking2chosen[1000:1200]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(eval_data))\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, 512)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f23354b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "## prompt ##\n",
      "흑고래의 무게는 어느 정도야\n",
      "######################################################################\n",
      "## chosen ##\n",
      "흑고래의 평균 몸무게는 약 25~40톤 정도이지만, 최대 몸무게는 50톤 이상에 이를 수 있습니다.\n",
      "######################################################################\n",
      "## rejected ##\n",
      "흑고래의 무게는 매우 다양하게 달라집니다. 약 200kg에서 10톤까지 달라질 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "print('#'*70)\n",
    "print('## prompt ##')\n",
    "print(train_data[idx]['prompt'])\n",
    "print('#'*70)\n",
    "print('## chosen ##')\n",
    "print(train_data[idx]['chosen'])\n",
    "print('#'*70)\n",
    "print('## rejected ##')\n",
    "print(train_data[idx]['rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a56c0aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RewardModelTrainer(model=model,\n",
    "                             strategy=NaiveStrategy(),\n",
    "                             optim=Adam(model.parameters(), lr=5e-5),\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=4,\n",
    "                             max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e8cac19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Train step of epoch 0:   0%|          | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:01<04:34,  1.10s/it]\u001b[A\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:01<04:34,  1.10s/it, loss=0.699]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:01<03:57,  1.05it/s, loss=0.699]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:01<03:57,  1.05it/s, loss=0.441]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:02<03:44,  1.10it/s, loss=0.441]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:02<03:44,  1.10it/s, loss=0.62] \u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:37,  1.13it/s, loss=0.62]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:37,  1.13it/s, loss=0.58]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:33,  1.15it/s, loss=0.58]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:33,  1.15it/s, loss=0.264]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:31,  1.16it/s, loss=0.264]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:31,  1.16it/s, loss=0.749]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 7/250 [00:06<03:29,  1.16it/s, loss=0.749]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 7/250 [00:06<03:29,  1.16it/s, loss=0.666]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 8/250 [00:07<03:27,  1.17it/s, loss=0.666]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 8/250 [00:07<03:27,  1.17it/s, loss=1.52] \u001b[A\n",
      "Train step of epoch 0:   4%|▎         | 9/250 [00:07<03:26,  1.17it/s, loss=1.52]\u001b[A\n",
      "Train step of epoch 0:   4%|▎         | 9/250 [00:07<03:26,  1.17it/s, loss=0.446]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 10/250 [00:08<03:25,  1.17it/s, loss=0.446]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 10/250 [00:08<03:25,  1.17it/s, loss=0.677]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 11/250 [00:09<03:24,  1.17it/s, loss=0.677]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 11/250 [00:09<03:24,  1.17it/s, loss=0.359]\u001b[A\n",
      "Train step of epoch 0:   5%|▍         | 12/250 [00:10<03:23,  1.17it/s, loss=0.359]\u001b[A\n",
      "Train step of epoch 0:   5%|▍         | 12/250 [00:10<03:23,  1.17it/s, loss=0.357]\u001b[A\n",
      "Train step of epoch 0:   5%|▌         | 13/250 [00:11<03:22,  1.17it/s, loss=0.357]\u001b[A\n",
      "Train step of epoch 0:   5%|▌         | 13/250 [00:11<03:22,  1.17it/s, loss=0.0974]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 14/250 [00:12<03:22,  1.17it/s, loss=0.0974]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 14/250 [00:12<03:22,  1.17it/s, loss=1.69]  \u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 15/250 [00:13<03:21,  1.17it/s, loss=1.69]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 15/250 [00:13<03:21,  1.17it/s, loss=0.671]\u001b[A\n",
      "Train step of epoch 0:   6%|▋         | 16/250 [00:13<03:20,  1.17it/s, loss=0.671]\u001b[A\n",
      "Train step of epoch 0:   6%|▋         | 16/250 [00:13<03:20,  1.17it/s, loss=1.58] \u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 17/250 [00:14<03:20,  1.16it/s, loss=1.58]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 17/250 [00:14<03:20,  1.16it/s, loss=0.858]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 18/250 [00:15<03:19,  1.16it/s, loss=0.858]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 18/250 [00:15<03:19,  1.16it/s, loss=0.494]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 19/250 [00:16<03:18,  1.16it/s, loss=0.494]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 19/250 [00:16<03:18,  1.16it/s, loss=1]    \u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 20/250 [00:17<03:17,  1.16it/s, loss=1]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 20/250 [00:17<03:17,  1.16it/s, loss=0.572]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 21/250 [00:18<03:16,  1.16it/s, loss=0.572]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 21/250 [00:18<03:16,  1.16it/s, loss=0.641]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 22/250 [00:19<03:16,  1.16it/s, loss=0.641]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 22/250 [00:19<03:16,  1.16it/s, loss=0.462]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 23/250 [00:19<03:15,  1.16it/s, loss=0.462]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 23/250 [00:19<03:15,  1.16it/s, loss=1.27] \u001b[A\n",
      "Train step of epoch 0:  10%|▉         | 24/250 [00:20<03:14,  1.16it/s, loss=1.27]\u001b[A\n",
      "Train step of epoch 0:  10%|▉         | 24/250 [00:20<03:14,  1.16it/s, loss=0.662]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 25/250 [00:21<03:14,  1.16it/s, loss=0.662]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 25/250 [00:21<03:14,  1.16it/s, loss=0.476]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 26/250 [00:22<03:13,  1.16it/s, loss=0.476]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 26/250 [00:22<03:13,  1.16it/s, loss=0.791]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 27/250 [00:23<03:12,  1.16it/s, loss=0.791]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 27/250 [00:23<03:12,  1.16it/s, loss=0.531]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 28/250 [00:24<03:11,  1.16it/s, loss=0.531]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 28/250 [00:24<03:11,  1.16it/s, loss=0.658]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 29/250 [00:25<03:11,  1.16it/s, loss=0.658]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 29/250 [00:25<03:11,  1.16it/s, loss=0.481]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 30/250 [00:25<03:10,  1.15it/s, loss=0.481]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 30/250 [00:26<03:10,  1.15it/s, loss=0.591]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 31/250 [00:26<03:10,  1.15it/s, loss=0.591]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 31/250 [00:26<03:10,  1.15it/s, loss=0.486]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 32/250 [00:27<03:09,  1.15it/s, loss=0.486]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 32/250 [00:27<03:09,  1.15it/s, loss=0.607]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 33/250 [00:28<03:08,  1.15it/s, loss=0.607]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 33/250 [00:28<03:08,  1.15it/s, loss=0.39] \u001b[A\n",
      "Train step of epoch 0:  14%|█▎        | 34/250 [00:29<03:07,  1.15it/s, loss=0.39]\u001b[A\n",
      "Train step of epoch 0:  14%|█▎        | 34/250 [00:29<03:07,  1.15it/s, loss=0.464]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 35/250 [00:30<03:07,  1.15it/s, loss=0.464]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 35/250 [00:30<03:07,  1.15it/s, loss=1.73] \u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 36/250 [00:31<03:06,  1.15it/s, loss=1.73]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 36/250 [00:31<03:06,  1.15it/s, loss=1.4] \u001b[A\n",
      "Train step of epoch 0:  15%|█▍        | 37/250 [00:32<03:05,  1.15it/s, loss=1.4]\u001b[A\n",
      "Train step of epoch 0:  15%|█▍        | 37/250 [00:32<03:05,  1.15it/s, loss=0.805]\u001b[A\n",
      "Train step of epoch 0:  15%|█▌        | 38/250 [00:32<03:05,  1.15it/s, loss=0.805]\u001b[A\n",
      "Train step of epoch 0:  15%|█▌        | 38/250 [00:33<03:05,  1.15it/s, loss=0.603]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 39/250 [00:33<03:04,  1.15it/s, loss=0.603]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 39/250 [00:33<03:04,  1.15it/s, loss=0.925]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 40/250 [00:34<03:03,  1.15it/s, loss=0.925]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 40/250 [00:34<03:03,  1.15it/s, loss=0.539]\u001b[A\n",
      "Train step of epoch 0:  16%|█▋        | 41/250 [00:35<03:02,  1.14it/s, loss=0.539]\u001b[A\n",
      "Train step of epoch 0:  16%|█▋        | 41/250 [00:35<03:02,  1.14it/s, loss=0.495]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 42/250 [00:36<03:02,  1.14it/s, loss=0.495]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 42/250 [00:36<03:02,  1.14it/s, loss=0.548]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 43/250 [00:37<03:01,  1.14it/s, loss=0.548]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 43/250 [00:37<03:01,  1.14it/s, loss=0.888]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 44/250 [00:38<03:00,  1.14it/s, loss=0.888]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 44/250 [00:38<03:00,  1.14it/s, loss=1.44] \u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 45/250 [00:39<02:59,  1.14it/s, loss=1.44]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 45/250 [00:39<02:59,  1.14it/s, loss=0.511]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 46/250 [00:39<02:59,  1.14it/s, loss=0.511]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 46/250 [00:40<02:59,  1.14it/s, loss=0.947]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 47/250 [00:40<02:58,  1.14it/s, loss=0.947]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 47/250 [00:40<02:58,  1.14it/s, loss=0.71] \u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 48/250 [00:41<02:57,  1.14it/s, loss=0.71]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 48/250 [00:41<02:57,  1.14it/s, loss=0.958]\u001b[A\n",
      "Train step of epoch 0:  20%|█▉        | 49/250 [00:42<02:56,  1.14it/s, loss=0.958]\u001b[A\n",
      "Train step of epoch 0:  20%|█▉        | 49/250 [00:42<02:56,  1.14it/s, loss=0.658]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 50/250 [00:43<02:56,  1.14it/s, loss=0.658]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 50/250 [00:43<02:56,  1.14it/s, loss=0.719]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 51/250 [00:44<02:55,  1.13it/s, loss=0.719]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 51/250 [00:44<02:55,  1.13it/s, loss=0.382]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 52/250 [00:45<02:54,  1.14it/s, loss=0.382]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 52/250 [00:45<02:54,  1.14it/s, loss=0.667]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 53/250 [00:46<02:53,  1.13it/s, loss=0.667]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 53/250 [00:46<02:53,  1.13it/s, loss=0.449]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 54/250 [00:47<02:53,  1.13it/s, loss=0.449]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 54/250 [00:47<02:53,  1.13it/s, loss=0.453]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 55/250 [00:47<02:52,  1.13it/s, loss=0.453]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 55/250 [00:47<02:52,  1.13it/s, loss=0.543]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 56/250 [00:48<02:51,  1.13it/s, loss=0.543]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 56/250 [00:48<02:51,  1.13it/s, loss=0.518]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 57/250 [00:49<02:50,  1.13it/s, loss=0.518]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 57/250 [00:49<02:50,  1.13it/s, loss=0.597]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 58/250 [00:50<02:49,  1.13it/s, loss=0.597]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 58/250 [00:50<02:49,  1.13it/s, loss=0.457]\u001b[A\n",
      "Train step of epoch 0:  24%|██▎       | 59/250 [00:51<02:48,  1.13it/s, loss=0.457]\u001b[A\n",
      "Train step of epoch 0:  24%|██▎       | 59/250 [00:51<02:48,  1.13it/s, loss=0.676]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 60/250 [00:52<02:48,  1.13it/s, loss=0.676]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 60/250 [00:52<02:48,  1.13it/s, loss=0.398]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 61/250 [00:53<02:47,  1.13it/s, loss=0.398]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 61/250 [00:53<02:47,  1.13it/s, loss=0.802]\u001b[A\n",
      "Train step of epoch 0:  25%|██▍       | 62/250 [00:54<02:46,  1.13it/s, loss=0.802]\u001b[A\n",
      "Train step of epoch 0:  25%|██▍       | 62/250 [00:54<02:46,  1.13it/s, loss=0.235]\u001b[A\n",
      "Train step of epoch 0:  25%|██▌       | 63/250 [00:55<02:45,  1.13it/s, loss=0.235]\u001b[A\n",
      "Train step of epoch 0:  25%|██▌       | 63/250 [00:55<02:45,  1.13it/s, loss=0.785]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 64/250 [00:55<02:44,  1.13it/s, loss=0.785]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 64/250 [00:55<02:44,  1.13it/s, loss=0.322]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 65/250 [00:56<02:43,  1.13it/s, loss=0.322]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 65/250 [00:56<02:43,  1.13it/s, loss=0.715]\u001b[A\n",
      "Train step of epoch 0:  26%|██▋       | 66/250 [00:57<02:42,  1.13it/s, loss=0.715]\u001b[A\n",
      "Train step of epoch 0:  26%|██▋       | 66/250 [00:57<02:42,  1.13it/s, loss=0.156]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 67/250 [00:58<02:41,  1.13it/s, loss=0.156]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 67/250 [00:58<02:41,  1.13it/s, loss=1.07] \u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 68/250 [00:59<02:40,  1.13it/s, loss=1.07]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 68/250 [00:59<02:40,  1.13it/s, loss=0.437]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 69/250 [01:00<02:39,  1.13it/s, loss=0.437]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 69/250 [01:00<02:39,  1.13it/s, loss=0.879]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 70/250 [01:01<02:38,  1.13it/s, loss=0.879]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 70/250 [01:01<02:38,  1.13it/s, loss=0.789]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 71/250 [01:02<02:37,  1.13it/s, loss=0.789]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 71/250 [01:02<02:37,  1.13it/s, loss=0.379]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 72/250 [01:02<02:36,  1.14it/s, loss=0.379]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 72/250 [01:02<02:36,  1.14it/s, loss=0.894]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 73/250 [01:03<02:35,  1.14it/s, loss=0.894]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 73/250 [01:03<02:35,  1.14it/s, loss=0.877]\u001b[A\n",
      "Train step of epoch 0:  30%|██▉       | 74/250 [01:04<02:34,  1.14it/s, loss=0.877]\u001b[A\n",
      "Train step of epoch 0:  30%|██▉       | 74/250 [01:04<02:34,  1.14it/s, loss=0.455]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 75/250 [01:05<02:33,  1.14it/s, loss=0.455]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 75/250 [01:05<02:33,  1.14it/s, loss=0.635]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 76/250 [01:06<02:32,  1.14it/s, loss=0.635]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 76/250 [01:06<02:32,  1.14it/s, loss=0.755]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 77/250 [01:07<02:31,  1.14it/s, loss=0.755]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 77/250 [01:07<02:31,  1.14it/s, loss=0.354]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 78/250 [01:08<02:30,  1.14it/s, loss=0.354]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 78/250 [01:08<02:30,  1.14it/s, loss=0.59] \u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 79/250 [01:09<02:29,  1.14it/s, loss=0.59]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 79/250 [01:09<02:29,  1.14it/s, loss=0.535]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 80/250 [01:09<02:28,  1.14it/s, loss=0.535]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 80/250 [01:09<02:28,  1.14it/s, loss=0.728]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 81/250 [01:10<02:27,  1.14it/s, loss=0.728]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 81/250 [01:10<02:27,  1.14it/s, loss=0.291]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 82/250 [01:11<02:27,  1.14it/s, loss=0.291]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 82/250 [01:11<02:27,  1.14it/s, loss=0.879]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 83/250 [01:12<02:26,  1.14it/s, loss=0.879]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 83/250 [01:12<02:26,  1.14it/s, loss=0.512]\u001b[A\n",
      "Train step of epoch 0:  34%|███▎      | 84/250 [01:13<02:25,  1.14it/s, loss=0.512]\u001b[A\n",
      "Train step of epoch 0:  34%|███▎      | 84/250 [01:13<02:25,  1.14it/s, loss=0.625]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 85/250 [01:14<02:24,  1.14it/s, loss=0.625]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 85/250 [01:14<02:24,  1.14it/s, loss=0.713]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 86/250 [01:15<02:23,  1.14it/s, loss=0.713]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 86/250 [01:15<02:23,  1.14it/s, loss=0.543]\u001b[A\n",
      "Train step of epoch 0:  35%|███▍      | 87/250 [01:16<02:22,  1.15it/s, loss=0.543]\u001b[A\n",
      "Train step of epoch 0:  35%|███▍      | 87/250 [01:16<02:22,  1.15it/s, loss=0.49] \u001b[A\n",
      "Train step of epoch 0:  35%|███▌      | 88/250 [01:16<02:21,  1.15it/s, loss=0.49]\u001b[A\n",
      "Train step of epoch 0:  35%|███▌      | 88/250 [01:16<02:21,  1.15it/s, loss=0.862]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 89/250 [01:17<02:20,  1.15it/s, loss=0.862]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 89/250 [01:17<02:20,  1.15it/s, loss=0.806]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 90/250 [01:18<02:19,  1.15it/s, loss=0.806]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 90/250 [01:18<02:19,  1.15it/s, loss=0.676]\u001b[A\n",
      "Train step of epoch 0:  36%|███▋      | 91/250 [01:19<02:18,  1.15it/s, loss=0.676]\u001b[A\n",
      "Train step of epoch 0:  36%|███▋      | 91/250 [01:19<02:18,  1.15it/s, loss=0.599]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 92/250 [01:20<02:17,  1.15it/s, loss=0.599]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 92/250 [01:20<02:17,  1.15it/s, loss=0.593]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 93/250 [01:21<02:16,  1.15it/s, loss=0.593]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 93/250 [01:21<02:16,  1.15it/s, loss=0.824]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 94/250 [01:22<02:15,  1.15it/s, loss=0.824]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 94/250 [01:22<02:15,  1.15it/s, loss=0.647]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 95/250 [01:23<02:14,  1.15it/s, loss=0.647]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 95/250 [01:23<02:14,  1.15it/s, loss=0.491]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 96/250 [01:23<02:14,  1.15it/s, loss=0.491]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 96/250 [01:23<02:14,  1.15it/s, loss=0.753]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 97/250 [01:24<02:13,  1.15it/s, loss=0.753]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 97/250 [01:24<02:13,  1.15it/s, loss=0.804]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 98/250 [01:25<02:12,  1.15it/s, loss=0.804]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 98/250 [01:25<02:12,  1.15it/s, loss=0.733]\u001b[A\n",
      "Train step of epoch 0:  40%|███▉      | 99/250 [01:26<02:11,  1.15it/s, loss=0.733]\u001b[A\n",
      "Train step of epoch 0:  40%|███▉      | 99/250 [01:26<02:11,  1.15it/s, loss=0.595]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 100/250 [01:27<02:10,  1.15it/s, loss=0.595]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 100/250 [01:27<02:10,  1.15it/s, loss=0.59] \u001b[A\n",
      "Train step of epoch 0:  40%|████      | 101/250 [01:28<02:09,  1.15it/s, loss=0.59]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 101/250 [01:28<02:09,  1.15it/s, loss=0.747]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 102/250 [01:29<02:08,  1.15it/s, loss=0.747]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 102/250 [01:29<02:08,  1.15it/s, loss=0.681]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 103/250 [01:30<02:07,  1.15it/s, loss=0.681]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 103/250 [01:30<02:07,  1.15it/s, loss=0.834]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 104/250 [01:30<02:06,  1.15it/s, loss=0.834]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 104/250 [01:30<02:06,  1.15it/s, loss=0.867]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 105/250 [01:31<02:06,  1.15it/s, loss=0.867]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 105/250 [01:31<02:06,  1.15it/s, loss=0.65] \u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 106/250 [01:32<02:05,  1.15it/s, loss=0.65]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 106/250 [01:32<02:05,  1.15it/s, loss=0.685]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 107/250 [01:33<02:04,  1.15it/s, loss=0.685]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 107/250 [01:33<02:04,  1.15it/s, loss=0.767]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 108/250 [01:34<02:03,  1.15it/s, loss=0.767]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 108/250 [01:34<02:03,  1.15it/s, loss=0.599]\u001b[A\n",
      "Train step of epoch 0:  44%|████▎     | 109/250 [01:35<02:02,  1.15it/s, loss=0.599]\u001b[A\n",
      "Train step of epoch 0:  44%|████▎     | 109/250 [01:35<02:02,  1.15it/s, loss=0.687]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 110/250 [01:36<02:01,  1.15it/s, loss=0.687]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 110/250 [01:36<02:01,  1.15it/s, loss=0.67] \u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 111/250 [01:36<02:00,  1.15it/s, loss=0.67]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 111/250 [01:36<02:00,  1.15it/s, loss=0.585]\u001b[A\n",
      "Train step of epoch 0:  45%|████▍     | 112/250 [01:37<01:59,  1.15it/s, loss=0.585]\u001b[A\n",
      "Train step of epoch 0:  45%|████▍     | 112/250 [01:37<01:59,  1.15it/s, loss=0.665]\u001b[A\n",
      "Train step of epoch 0:  45%|████▌     | 113/250 [01:38<01:58,  1.15it/s, loss=0.665]\u001b[A\n",
      "Train step of epoch 0:  45%|████▌     | 113/250 [01:38<01:58,  1.15it/s, loss=0.703]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 114/250 [01:39<01:58,  1.15it/s, loss=0.703]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 114/250 [01:39<01:58,  1.15it/s, loss=0.906]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 115/250 [01:40<01:57,  1.15it/s, loss=0.906]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 115/250 [01:40<01:57,  1.15it/s, loss=0.693]\u001b[A\n",
      "Train step of epoch 0:  46%|████▋     | 116/250 [01:41<01:56,  1.15it/s, loss=0.693]\u001b[A\n",
      "Train step of epoch 0:  46%|████▋     | 116/250 [01:41<01:56,  1.15it/s, loss=0.594]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 117/250 [01:42<01:55,  1.15it/s, loss=0.594]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 117/250 [01:42<01:55,  1.15it/s, loss=0.538]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 118/250 [01:43<01:54,  1.15it/s, loss=0.538]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 118/250 [01:43<01:54,  1.15it/s, loss=0.524]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 119/250 [01:43<01:53,  1.15it/s, loss=0.524]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 119/250 [01:43<01:53,  1.15it/s, loss=0.511]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 120/250 [01:44<01:52,  1.15it/s, loss=0.511]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 120/250 [01:44<01:52,  1.15it/s, loss=1.07] \u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 121/250 [01:45<01:52,  1.15it/s, loss=1.07]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 121/250 [01:45<01:52,  1.15it/s, loss=0.795]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 122/250 [01:46<01:51,  1.15it/s, loss=0.795]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 122/250 [01:46<01:51,  1.15it/s, loss=0.689]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 123/250 [01:47<01:50,  1.15it/s, loss=0.689]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 123/250 [01:47<01:50,  1.15it/s, loss=0.535]\u001b[A\n",
      "Train step of epoch 0:  50%|████▉     | 124/250 [01:48<01:49,  1.15it/s, loss=0.535]\u001b[A\n",
      "Train step of epoch 0:  50%|████▉     | 124/250 [01:48<01:49,  1.15it/s, loss=0.698]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 125/250 [01:49<01:48,  1.15it/s, loss=0.698]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 125/250 [01:49<01:48,  1.15it/s, loss=0.666]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 126/250 [01:49<01:48,  1.15it/s, loss=0.666]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 126/250 [01:50<01:48,  1.15it/s, loss=0.666]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 127/250 [01:50<01:47,  1.15it/s, loss=0.666]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 127/250 [01:50<01:47,  1.15it/s, loss=0.6]  \u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 128/250 [01:51<01:46,  1.15it/s, loss=0.6]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 128/250 [01:51<01:46,  1.15it/s, loss=0.653]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 129/250 [01:52<01:45,  1.15it/s, loss=0.653]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 129/250 [01:52<01:45,  1.15it/s, loss=0.629]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 130/250 [01:53<01:44,  1.15it/s, loss=0.629]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 130/250 [01:53<01:44,  1.15it/s, loss=0.617]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 131/250 [01:54<01:43,  1.15it/s, loss=0.617]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 131/250 [01:54<01:43,  1.15it/s, loss=0.842]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 132/250 [01:55<01:42,  1.15it/s, loss=0.842]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 132/250 [01:55<01:42,  1.15it/s, loss=0.592]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 133/250 [01:56<01:41,  1.15it/s, loss=0.592]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 133/250 [01:56<01:41,  1.15it/s, loss=0.773]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▎    | 134/250 [01:56<01:40,  1.15it/s, loss=0.773]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▎    | 134/250 [01:56<01:40,  1.15it/s, loss=0.664]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 135/250 [01:57<01:40,  1.15it/s, loss=0.664]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 135/250 [01:57<01:40,  1.15it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 136/250 [01:58<01:39,  1.15it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 136/250 [01:58<01:39,  1.15it/s, loss=0.635]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▍    | 137/250 [01:59<01:38,  1.15it/s, loss=0.635]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▍    | 137/250 [01:59<01:38,  1.15it/s, loss=0.537]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▌    | 138/250 [02:00<01:37,  1.15it/s, loss=0.537]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▌    | 138/250 [02:00<01:37,  1.15it/s, loss=0.657]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 139/250 [02:01<01:36,  1.15it/s, loss=0.657]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 139/250 [02:01<01:36,  1.15it/s, loss=0.517]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 140/250 [02:02<01:35,  1.15it/s, loss=0.517]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 140/250 [02:02<01:35,  1.15it/s, loss=0.489]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▋    | 141/250 [02:03<01:34,  1.15it/s, loss=0.489]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▋    | 141/250 [02:03<01:34,  1.15it/s, loss=0.629]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 142/250 [02:03<01:34,  1.15it/s, loss=0.629]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 142/250 [02:03<01:34,  1.15it/s, loss=0.846]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 143/250 [02:04<01:33,  1.15it/s, loss=0.846]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 143/250 [02:04<01:33,  1.15it/s, loss=0.581]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 144/250 [02:05<01:32,  1.15it/s, loss=0.581]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 144/250 [02:05<01:32,  1.15it/s, loss=0.475]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 145/250 [02:06<01:31,  1.15it/s, loss=0.475]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 145/250 [02:06<01:31,  1.15it/s, loss=0.71] \u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 146/250 [02:07<01:30,  1.14it/s, loss=0.71]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 146/250 [02:07<01:30,  1.14it/s, loss=0.7] \u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 147/250 [02:08<01:29,  1.14it/s, loss=0.7]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 147/250 [02:08<01:29,  1.14it/s, loss=0.391]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 148/250 [02:09<01:29,  1.14it/s, loss=0.391]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 148/250 [02:09<01:29,  1.14it/s, loss=0.64] \u001b[A\n",
      "Train step of epoch 0:  60%|█████▉    | 149/250 [02:10<01:28,  1.14it/s, loss=0.64]\u001b[A\n",
      "Train step of epoch 0:  60%|█████▉    | 149/250 [02:10<01:28,  1.14it/s, loss=0.719]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 150/250 [02:10<01:27,  1.14it/s, loss=0.719]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 150/250 [02:10<01:27,  1.14it/s, loss=0.603]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 151/250 [02:11<01:26,  1.14it/s, loss=0.603]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 151/250 [02:11<01:26,  1.14it/s, loss=0.334]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 152/250 [02:12<01:25,  1.14it/s, loss=0.334]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 152/250 [02:12<01:25,  1.14it/s, loss=0.533]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 153/250 [02:13<01:24,  1.14it/s, loss=0.533]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 153/250 [02:13<01:24,  1.14it/s, loss=0.676]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 154/250 [02:14<01:24,  1.14it/s, loss=0.676]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 154/250 [02:14<01:24,  1.14it/s, loss=1.03] \u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 155/250 [02:15<01:23,  1.14it/s, loss=1.03]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 155/250 [02:15<01:23,  1.14it/s, loss=1.04]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 156/250 [02:16<01:22,  1.14it/s, loss=1.04]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 156/250 [02:16<01:22,  1.14it/s, loss=0.464]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 157/250 [02:17<01:21,  1.14it/s, loss=0.464]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 157/250 [02:17<01:21,  1.14it/s, loss=0.563]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 158/250 [02:17<01:20,  1.14it/s, loss=0.563]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 158/250 [02:17<01:20,  1.14it/s, loss=0.662]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▎   | 159/250 [02:18<01:19,  1.14it/s, loss=0.662]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▎   | 159/250 [02:18<01:19,  1.14it/s, loss=0.78] \u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 160/250 [02:19<01:18,  1.14it/s, loss=0.78]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 160/250 [02:19<01:18,  1.14it/s, loss=0.447]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 161/250 [02:20<01:18,  1.14it/s, loss=0.447]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 161/250 [02:20<01:18,  1.14it/s, loss=0.783]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▍   | 162/250 [02:21<01:17,  1.14it/s, loss=0.783]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▍   | 162/250 [02:21<01:17,  1.14it/s, loss=0.533]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▌   | 163/250 [02:22<01:16,  1.14it/s, loss=0.533]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▌   | 163/250 [02:22<01:16,  1.14it/s, loss=0.435]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 164/250 [02:23<01:15,  1.14it/s, loss=0.435]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 164/250 [02:23<01:15,  1.14it/s, loss=0.554]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 165/250 [02:24<01:14,  1.14it/s, loss=0.554]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 165/250 [02:24<01:14,  1.14it/s, loss=1.04] \u001b[A\n",
      "Train step of epoch 0:  66%|██████▋   | 166/250 [02:24<01:13,  1.14it/s, loss=1.04]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▋   | 166/250 [02:24<01:13,  1.14it/s, loss=0.31]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 167/250 [02:25<01:12,  1.14it/s, loss=0.31]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 167/250 [02:25<01:12,  1.14it/s, loss=0.396]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 168/250 [02:26<01:11,  1.14it/s, loss=0.396]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 168/250 [02:26<01:11,  1.14it/s, loss=0.512]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 169/250 [02:27<01:10,  1.14it/s, loss=0.512]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 169/250 [02:27<01:10,  1.14it/s, loss=0.386]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 170/250 [02:28<01:10,  1.14it/s, loss=0.386]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 170/250 [02:28<01:10,  1.14it/s, loss=0.846]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 171/250 [02:29<01:09,  1.14it/s, loss=0.846]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 171/250 [02:29<01:09,  1.14it/s, loss=0.714]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 172/250 [02:30<01:08,  1.14it/s, loss=0.714]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 172/250 [02:30<01:08,  1.14it/s, loss=0.859]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 173/250 [02:31<01:07,  1.14it/s, loss=0.859]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 173/250 [02:31<01:07,  1.14it/s, loss=0.441]\u001b[A\n",
      "Train step of epoch 0:  70%|██████▉   | 174/250 [02:31<01:06,  1.14it/s, loss=0.441]\u001b[A\n",
      "Train step of epoch 0:  70%|██████▉   | 174/250 [02:31<01:06,  1.14it/s, loss=0.296]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 175/250 [02:32<01:05,  1.14it/s, loss=0.296]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 175/250 [02:32<01:05,  1.14it/s, loss=0.601]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 176/250 [02:33<01:04,  1.14it/s, loss=0.601]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 176/250 [02:33<01:04,  1.14it/s, loss=0.424]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 177/250 [02:34<01:03,  1.14it/s, loss=0.424]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 177/250 [02:34<01:03,  1.14it/s, loss=0.403]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 178/250 [02:35<01:03,  1.14it/s, loss=0.403]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 178/250 [02:35<01:03,  1.14it/s, loss=0.385]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 179/250 [02:36<01:02,  1.14it/s, loss=0.385]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 179/250 [02:36<01:02,  1.14it/s, loss=0.524]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 180/250 [02:37<01:01,  1.14it/s, loss=0.524]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 180/250 [02:37<01:01,  1.14it/s, loss=0.646]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 181/250 [02:38<01:00,  1.14it/s, loss=0.646]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 181/250 [02:38<01:00,  1.14it/s, loss=0.296]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 182/250 [02:38<00:59,  1.15it/s, loss=0.296]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 182/250 [02:38<00:59,  1.15it/s, loss=1.21] \u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 183/250 [02:39<00:58,  1.15it/s, loss=1.21]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 183/250 [02:39<00:58,  1.15it/s, loss=1.2] \u001b[A\n",
      "Train step of epoch 0:  74%|███████▎  | 184/250 [02:40<00:57,  1.14it/s, loss=1.2]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▎  | 184/250 [02:40<00:57,  1.14it/s, loss=0.76]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 185/250 [02:41<00:56,  1.14it/s, loss=0.76]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 185/250 [02:41<00:56,  1.14it/s, loss=0.621]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 186/250 [02:42<00:55,  1.14it/s, loss=0.621]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 186/250 [02:42<00:55,  1.14it/s, loss=0.367]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▍  | 187/250 [02:43<00:55,  1.14it/s, loss=0.367]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▍  | 187/250 [02:43<00:55,  1.14it/s, loss=0.322]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▌  | 188/250 [02:44<00:54,  1.14it/s, loss=0.322]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▌  | 188/250 [02:44<00:54,  1.14it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 189/250 [02:45<00:53,  1.14it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 189/250 [02:45<00:53,  1.14it/s, loss=0.826]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 190/250 [02:45<00:52,  1.14it/s, loss=0.826]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 190/250 [02:45<00:52,  1.14it/s, loss=0.377]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▋  | 191/250 [02:46<00:51,  1.14it/s, loss=0.377]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▋  | 191/250 [02:46<00:51,  1.14it/s, loss=0.284]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 192/250 [02:47<00:50,  1.14it/s, loss=0.284]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 192/250 [02:47<00:50,  1.14it/s, loss=0.513]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 193/250 [02:48<00:49,  1.14it/s, loss=0.513]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 193/250 [02:48<00:49,  1.14it/s, loss=0.946]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 194/250 [02:49<00:48,  1.14it/s, loss=0.946]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 194/250 [02:49<00:48,  1.14it/s, loss=0.801]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 195/250 [02:50<00:48,  1.14it/s, loss=0.801]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 195/250 [02:50<00:48,  1.14it/s, loss=0.378]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 196/250 [02:51<00:47,  1.14it/s, loss=0.378]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 196/250 [02:51<00:47,  1.14it/s, loss=1.02] \u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 197/250 [02:52<00:46,  1.14it/s, loss=1.02]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 197/250 [02:52<00:46,  1.14it/s, loss=0.558]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 198/250 [02:52<00:45,  1.15it/s, loss=0.558]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 198/250 [02:52<00:45,  1.15it/s, loss=0.519]\u001b[A\n",
      "Train step of epoch 0:  80%|███████▉  | 199/250 [02:53<00:44,  1.14it/s, loss=0.519]\u001b[A\n",
      "Train step of epoch 0:  80%|███████▉  | 199/250 [02:53<00:44,  1.14it/s, loss=0.414]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 200/250 [02:54<00:43,  1.14it/s, loss=0.414]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 200/250 [02:54<00:43,  1.14it/s, loss=0.474]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 201/250 [02:55<00:42,  1.14it/s, loss=0.474]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 201/250 [02:55<00:42,  1.14it/s, loss=0.492]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 202/250 [02:56<00:41,  1.14it/s, loss=0.492]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 202/250 [02:56<00:41,  1.14it/s, loss=0.867]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 203/250 [02:57<00:41,  1.14it/s, loss=0.867]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 203/250 [02:57<00:41,  1.14it/s, loss=0.564]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 204/250 [02:58<00:40,  1.14it/s, loss=0.564]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 204/250 [02:58<00:40,  1.14it/s, loss=0.463]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 205/250 [02:59<00:39,  1.14it/s, loss=0.463]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 205/250 [02:59<00:39,  1.14it/s, loss=0.651]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 206/250 [02:59<00:38,  1.14it/s, loss=0.651]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 206/250 [02:59<00:38,  1.14it/s, loss=0.74] \u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 207/250 [03:00<00:37,  1.14it/s, loss=0.74]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 207/250 [03:00<00:37,  1.14it/s, loss=0.496]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 208/250 [03:01<00:36,  1.15it/s, loss=0.496]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 208/250 [03:01<00:36,  1.15it/s, loss=0.682]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▎ | 209/250 [03:02<00:35,  1.15it/s, loss=0.682]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▎ | 209/250 [03:02<00:35,  1.15it/s, loss=0.671]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 210/250 [03:03<00:34,  1.15it/s, loss=0.671]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 210/250 [03:03<00:34,  1.15it/s, loss=0.633]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 211/250 [03:04<00:33,  1.15it/s, loss=0.633]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 211/250 [03:04<00:33,  1.15it/s, loss=0.448]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▍ | 212/250 [03:05<00:33,  1.15it/s, loss=0.448]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▍ | 212/250 [03:05<00:33,  1.15it/s, loss=0.58] \u001b[A\n",
      "Train step of epoch 0:  85%|████████▌ | 213/250 [03:06<00:32,  1.15it/s, loss=0.58]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▌ | 213/250 [03:06<00:32,  1.15it/s, loss=0.513]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 214/250 [03:06<00:31,  1.15it/s, loss=0.513]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 214/250 [03:06<00:31,  1.15it/s, loss=1.04] \u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 215/250 [03:07<00:30,  1.14it/s, loss=1.04]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 215/250 [03:07<00:30,  1.14it/s, loss=0.478]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▋ | 216/250 [03:08<00:29,  1.14it/s, loss=0.478]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▋ | 216/250 [03:08<00:29,  1.14it/s, loss=0.823]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 217/250 [03:09<00:28,  1.14it/s, loss=0.823]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 217/250 [03:09<00:28,  1.14it/s, loss=0.891]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 218/250 [03:10<00:27,  1.15it/s, loss=0.891]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 218/250 [03:10<00:27,  1.15it/s, loss=0.557]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 219/250 [03:11<00:27,  1.14it/s, loss=0.557]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 219/250 [03:11<00:27,  1.14it/s, loss=0.395]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 220/250 [03:12<00:26,  1.15it/s, loss=0.395]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 220/250 [03:12<00:26,  1.15it/s, loss=0.679]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 221/250 [03:12<00:25,  1.15it/s, loss=0.679]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 221/250 [03:13<00:25,  1.15it/s, loss=0.639]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 222/250 [03:13<00:24,  1.15it/s, loss=0.639]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 222/250 [03:13<00:24,  1.15it/s, loss=0.338]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 223/250 [03:14<00:23,  1.15it/s, loss=0.338]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 223/250 [03:14<00:23,  1.15it/s, loss=0.647]\u001b[A\n",
      "Train step of epoch 0:  90%|████████▉ | 224/250 [03:15<00:22,  1.15it/s, loss=0.647]\u001b[A\n",
      "Train step of epoch 0:  90%|████████▉ | 224/250 [03:15<00:22,  1.15it/s, loss=0.765]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 225/250 [03:16<00:21,  1.15it/s, loss=0.765]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 225/250 [03:16<00:21,  1.15it/s, loss=0.571]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 226/250 [03:17<00:20,  1.15it/s, loss=0.571]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 226/250 [03:17<00:20,  1.15it/s, loss=0.651]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 227/250 [03:18<00:20,  1.15it/s, loss=0.651]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 227/250 [03:18<00:20,  1.15it/s, loss=0.717]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 228/250 [03:19<00:19,  1.14it/s, loss=0.717]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 228/250 [03:19<00:19,  1.14it/s, loss=0.556]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 229/250 [03:19<00:18,  1.15it/s, loss=0.556]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 229/250 [03:19<00:18,  1.15it/s, loss=0.703]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 230/250 [03:20<00:17,  1.14it/s, loss=0.703]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 230/250 [03:20<00:17,  1.14it/s, loss=0.702]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 231/250 [03:21<00:16,  1.14it/s, loss=0.702]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 231/250 [03:21<00:16,  1.14it/s, loss=0.429]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 232/250 [03:22<00:15,  1.14it/s, loss=0.429]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 232/250 [03:22<00:15,  1.14it/s, loss=0.808]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 233/250 [03:23<00:14,  1.14it/s, loss=0.808]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 233/250 [03:23<00:14,  1.14it/s, loss=0.452]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▎| 234/250 [03:24<00:14,  1.14it/s, loss=0.452]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▎| 234/250 [03:24<00:14,  1.14it/s, loss=0.483]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 235/250 [03:25<00:13,  1.14it/s, loss=0.483]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 235/250 [03:25<00:13,  1.14it/s, loss=0.901]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 236/250 [03:26<00:12,  1.14it/s, loss=0.901]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 236/250 [03:26<00:12,  1.14it/s, loss=0.481]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▍| 237/250 [03:26<00:11,  1.15it/s, loss=0.481]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▍| 237/250 [03:26<00:11,  1.15it/s, loss=0.566]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▌| 238/250 [03:27<00:10,  1.15it/s, loss=0.566]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▌| 238/250 [03:27<00:10,  1.15it/s, loss=0.613]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 239/250 [03:28<00:09,  1.15it/s, loss=0.613]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 239/250 [03:28<00:09,  1.15it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 240/250 [03:29<00:08,  1.15it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 240/250 [03:29<00:08,  1.15it/s, loss=0.535]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▋| 241/250 [03:30<00:07,  1.15it/s, loss=0.535]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▋| 241/250 [03:30<00:07,  1.15it/s, loss=0.562]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 242/250 [03:31<00:06,  1.15it/s, loss=0.562]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 242/250 [03:31<00:06,  1.15it/s, loss=0.859]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 243/250 [03:32<00:06,  1.15it/s, loss=0.859]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 243/250 [03:32<00:06,  1.15it/s, loss=0.39] \u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 244/250 [03:33<00:05,  1.15it/s, loss=0.39]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 244/250 [03:33<00:05,  1.15it/s, loss=0.659]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 245/250 [03:33<00:04,  1.14it/s, loss=0.659]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 245/250 [03:33<00:04,  1.14it/s, loss=0.922]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 246/250 [03:34<00:03,  1.14it/s, loss=0.922]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 246/250 [03:34<00:03,  1.14it/s, loss=0.982]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 247/250 [03:35<00:02,  1.15it/s, loss=0.982]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 247/250 [03:35<00:02,  1.15it/s, loss=0.373]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 248/250 [03:36<00:01,  1.15it/s, loss=0.373]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 248/250 [03:36<00:01,  1.15it/s, loss=1.67] \u001b[A\n",
      "Train step of epoch 0: 100%|█████████▉| 249/250 [03:37<00:00,  1.15it/s, loss=1.67]\u001b[A\n",
      "Train step of epoch 0: 100%|█████████▉| 249/250 [03:37<00:00,  1.15it/s, loss=0.638]\u001b[A\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [03:38<00:00,  1.15it/s, loss=0.638]\u001b[A\n",
      "Train epoch: 100%|██████████| 1/1 [03:52<00:00, 232.62s/it]0,  1.15it/s, loss=0.534]\u001b[A\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [03:52<00:00,  1.07it/s, loss=0.63, dist_mean=0.281]\u001b[A\n",
      "Train epoch: 100%|██████████| 1/1 [03:52<00:00, 232.63s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(use_lora=0)\n",
    "\n",
    "model.save_pretrained('aiffel/KoChatGPT/output_2_RM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3e75176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능은 똥멍청이 입니다\n",
      "reward score: 0.4\n"
     ]
    }
   ],
   "source": [
    "def inference_RM(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "\n",
    "    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n",
    "\n",
    "    return output_reward\n",
    "\n",
    "input_text = '인공지능은 똥멍청이 입니다'\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7b814d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.\n",
      "reward score: 1.8\n"
     ]
    }
   ],
   "source": [
    "input_text = '인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.'\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a75040dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다. AI는 현대적인 컴퓨팅 혁신에서 중추적인 역할을 하며 개인과 비즈니스의 가치를 창출합니다. 예를 들어 광학 문자 인식(OCR)은 AI를 사용해 이미지 및 문서에서 텍스트 및 데이터를 추출하고, 구조화되지 않은 콘텐츠를 비즈니스에 바로 사용할 수 있게 만들고, 유용한 정보를 창출합니다.\n",
      "reward score: 1.9\n"
     ]
    }
   ],
   "source": [
    "input_text = \"인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다. AI는 현대적인 컴퓨팅 혁신에서 중추적인 역할을 하며 개인과 비즈니스의 가치를 창출합니다. 예를 들어 광학 문자 인식(OCR)은 AI를 사용해 이미지 및 문서에서 텍스트 및 데이터를 추출하고, 구조화되지 않은 콘텐츠를 비즈니스에 바로 사용할 수 있게 만들고, 유용한 정보를 창출합니다.\"\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f52d7236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능은 일반적으로 인간의 지능이 필요하거나 인간이 분석할 수 있는 것보다 규모가 큰 데이터를 포함하는 방식으로 추론, 학습 및 행동할 수 있는 컴퓨터 및 기계를 구축하는 것과 관련된 과학 분야입니다. AI는 컴퓨터 공학, 데이터 분석 및 통계, 하드웨어 및 소프트웨어 엔지니어링, 언어학, 신경 과학은 물론 철학과 심리학을 포함하여 여러 학문을 포괄하는 광범위한 분야입니다. 비즈니스의 운영 수준에서 AI는 주로 머신러닝과 딥 러닝을 기반으로 하는 기술 모음으로, 데이터 분석, 예상 및 예측, 객체 분류, 자연어 처리, 추천, 지능형 데이터 가져오기 등을 수행할 수 있습니다.\n",
      "reward score: 1.4\n"
     ]
    }
   ],
   "source": [
    "input_text = \"인공지능은 일반적으로 인간의 지능이 필요하거나 인간이 분석할 수 있는 것보다 규모가 큰 데이터를 포함하는 방식으로 추론, 학습 및 행동할 수 있는 컴퓨터 및 기계를 구축하는 것과 관련된 과학 분야입니다. AI는 컴퓨터 공학, 데이터 분석 및 통계, 하드웨어 및 소프트웨어 엔지니어링, 언어학, 신경 과학은 물론 철학과 심리학을 포함하여 여러 학문을 포괄하는 광범위한 분야입니다. 비즈니스의 운영 수준에서 AI는 주로 머신러닝과 딥 러닝을 기반으로 하는 기술 모음으로, 데이터 분석, 예상 및 예측, 객체 분류, 자연어 처리, 추천, 지능형 데이터 가져오기 등을 수행할 수 있습니다.\"\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "56e68474",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf6fb5b",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ceaae0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ac10652",
   "metadata": {},
   "outputs": [],
   "source": [
    "with NaiveStrategy().model_init_context():\n",
    "    actor = GPTActor(pretrained='../KoChatGPT/output_1_SFT', lora_rank=0).to(torch.cuda.current_device())\n",
    "    critic = GPTCritic(pretrained='../KoChatGPT/output_2_RM', lora_rank=0).to(torch.cuda.current_device())\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", \n",
    "        model_max_length=512\n",
    "    )\n",
    "\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bd8e45ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = Adam(critic.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf93dd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6bda2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bed056af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c10928b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[47311, 10448, 19008,  9792, 11780, 11308, 30190, 10929, 11849, 21663,\n",
      "         44389,  9574, 13799,   458, 14308, 12778, 22469, 20938, 44696,   458,\n",
      "         13799,   458, 14308, 12778, 11756, 18944,   389]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_fn('It takes something more than intelligence to act intelligently.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dfebaca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6621ef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(NaiveStrategy(),\n",
    "                     actor,\n",
    "                     critic,\n",
    "                     reward_model,\n",
    "                     initial_model,\n",
    "                     actor_optim,\n",
    "                     critic_optim,\n",
    "                     max_epochs=1,  \n",
    "                     train_batch_size=8, \n",
    "                     tokenizer=tokenize_fn,\n",
    "                     max_length=128,\n",
    "                     do_sample=True,\n",
    "                     temperature=1.0,\n",
    "                     top_k=50,\n",
    "                     pad_token_id=tokenizer.pad_token_id,\n",
    "                     eos_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9a00d3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode [1/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.74s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.000321]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.60it/s, actor_loss=0, critic_loss=0.000321]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.60it/s, actor_loss=0, critic_loss=0.119]   \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s, actor_loss=0, critic_loss=0.119]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.79it/s, actor_loss=0, critic_loss=0.0083]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.81it/s, actor_loss=0, critic_loss=0.0083]\u001b[A\n",
      "Episode [1/10]: 100%|██████████| 3/3 [00:18<00:00,  6.26s/it]\n",
      "Episode [2/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.77s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.141, critic_loss=0.0284]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.88it/s, actor_loss=-.141, critic_loss=0.0284]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.88it/s, actor_loss=-.127, critic_loss=0.0641]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=-.127, critic_loss=0.0641]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=-.134, critic_loss=0.0543]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.87it/s, actor_loss=-.134, critic_loss=0.0543]\u001b[A\n",
      "Episode [2/10]: 100%|██████████| 3/3 [00:18<00:00,  6.30s/it]\n",
      "Episode [3/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.94s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0946, critic_loss=0.0149]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s, actor_loss=-.0946, critic_loss=0.0149]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.85it/s, actor_loss=-.0932, critic_loss=0.000515]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=-.0932, critic_loss=0.000515]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=-.0947, critic_loss=0.0155]  \u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.86it/s, actor_loss=-.0947, critic_loss=0.0155]\u001b[A\n",
      "Episode [3/10]: 100%|██████████| 3/3 [00:19<00:00,  6.48s/it]\n",
      "Episode [4/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.91s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.205, critic_loss=0.0335]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.88it/s, actor_loss=0.205, critic_loss=0.0335]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.88it/s, actor_loss=0.196, critic_loss=0.0317]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.89it/s, actor_loss=0.196, critic_loss=0.0317]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.89it/s, actor_loss=0.208, critic_loss=0.0159]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.88it/s, actor_loss=0.208, critic_loss=0.0159]\u001b[A\n",
      "Episode [4/10]: 100%|██████████| 3/3 [00:19<00:00,  6.40s/it]\n",
      "Episode [5/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.73s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.111, critic_loss=0.0146]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.91it/s, actor_loss=0.111, critic_loss=0.0146]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.91it/s, actor_loss=0.247, critic_loss=0.162] \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s, actor_loss=0.247, critic_loss=0.162]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.91it/s, actor_loss=0.0997, critic_loss=0.0192]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.90it/s, actor_loss=0.0997, critic_loss=0.0192]\u001b[A\n",
      "Episode [5/10]: 100%|██████████| 3/3 [00:18<00:00,  6.29s/it]\n",
      "Episode [6/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.73s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.172, critic_loss=0.0478]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.88it/s, actor_loss=-.172, critic_loss=0.0478]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.88it/s, actor_loss=-.143, critic_loss=0.0459]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.89it/s, actor_loss=-.143, critic_loss=0.0459]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.89it/s, actor_loss=-.185, critic_loss=0.0419]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.88it/s, actor_loss=-.185, critic_loss=0.0419]\u001b[A\n",
      "Episode [6/10]: 100%|██████████| 3/3 [00:18<00:00,  6.30s/it]\n",
      "Episode [7/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.79s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.103, critic_loss=0.0188]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.87it/s, actor_loss=-.103, critic_loss=0.0188]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.87it/s, actor_loss=-.107, critic_loss=0.00194]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.89it/s, actor_loss=-.107, critic_loss=0.00194]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.89it/s, actor_loss=-.095, critic_loss=0.00758]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.88it/s, actor_loss=-.095, critic_loss=0.00758]\u001b[A\n",
      "Episode [7/10]: 100%|██████████| 3/3 [00:19<00:00,  6.34s/it]\n",
      "Episode [8/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.79s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.219, critic_loss=0.042]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.89it/s, actor_loss=0.219, critic_loss=0.042]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.89it/s, actor_loss=0.331, critic_loss=0.064]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s, actor_loss=0.331, critic_loss=0.064]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.90it/s, actor_loss=0.14, critic_loss=0.0167]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.89it/s, actor_loss=0.14, critic_loss=0.0167]\u001b[A\n",
      "Episode [8/10]: 100%|██████████| 3/3 [00:19<00:00,  6.33s/it]\n",
      "Episode [9/10]:  67%|██████▋   | 2/3 [00:10<00:05,  5.16s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.108, critic_loss=0.00781]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.86it/s, actor_loss=0.108, critic_loss=0.00781]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.86it/s, actor_loss=0.235, critic_loss=0.113]  \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=0.235, critic_loss=0.113]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=0.179, critic_loss=0.0115]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.87it/s, actor_loss=0.179, critic_loss=0.0115]\u001b[A\n",
      "Episode [9/10]: 100%|██████████| 3/3 [00:17<00:00,  5.94s/it]\n",
      "Episode [10/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.78s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0456, critic_loss=0.0152]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:00<00:01,  1.90it/s, actor_loss=-.0456, critic_loss=0.0152]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 1/3 [00:01<00:01,  1.90it/s, actor_loss=-.121, critic_loss=0.0423] \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.89it/s, actor_loss=-.121, critic_loss=0.0423]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 2/3 [00:01<00:00,  1.89it/s, actor_loss=-.125, critic_loss=0.0378]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 3/3 [00:01<00:00,  1.88it/s, actor_loss=-.125, critic_loss=0.0378]\u001b[A\n",
      "Episode [10/10]: 100%|██████████| 3/3 [00:17<00:00,  5.82s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(list_prompt, \n",
    "            num_episodes=10,  \n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)\n",
    "\n",
    "model.save_pretrained('aiffel/KoChatGPT/output_3_PPO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5886f3e3",
   "metadata": {},
   "source": [
    "# 결과 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a625b09",
   "metadata": {},
   "source": [
    "## SFT,RM,PPO 적용 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a3fbdeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'죄송합니다, 저는 언어모델로써 대답만 가능하므로, \"불고기용 고기 한우에요?\"라는 문장은 어디서 사용되는 것인가요? 좀 더 자세한 정보를 알려주시면 정확한 답변을 드릴 수 있습니다. 달걀의 상태와 지역에 따라 다릅니다. 추가 정보를 제공해주시면 감사하겠습니다. 다진고기용 고기를 지칭한 경우 지역센터나 고기를 판매하는 가게에서 구매하실 수 있습니다.버스나 기타 다른 문항들을 활용하시면\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'리처드 닉슨이 41대 부통령직을 수행한 년도는 1951년입니다. 보면, 닉슨은 남부를 담당했으며, 41대 부통령을 한 번에 수행한 경험이 있습니다. 보면, 닉슨은 43대 부통령직을 수행한 적이 있습니다. 보면, 닉슨은 46대 부통령직을 맡았던 적이 있습니다. 보면, 39대 부통령직을 수행한 역사적인 인물들의 기록을 확인해야 합니다. Canada에서 가져온 인용한 것으로, 총 133명의 인물의 프로필 사진을 밝혔습니다.son에서 가져온 역사적인물 목록은 \"주\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### Response(응답):'시카고 오헤어 국제공항은 미국 루이지애나 주 시카고 시에 위치해 있습니다. 프로테우스(Thromptericus)는 시카고의 중심지로, 미국-시카고 협정을 기념하여 지어졌습니다. 또한 세계 최초의 공항이 되었으며, 현재는 미국내 유일한 국제공항으로 인정받고 있습니다.官新聞日報)는 미국의 대표적인 국제공항이다.砂回郎島外高察官際共和新聞報)는 1901년에 설립되었다.注題:省長刊官題.\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'미세먼지와 미세먼지 모두 미세먼지 농도는 좋습니다. 보통 고농도 미세먼지는 국내 대기오염으로 인한 대기오염을 유발시키며, 미세먼지 농도는 높게 나타나고 있습니다. 따라서 미세먼지와 대기질은 항상 유지되어야 하며, 필요하다면 미세먼지 농도는 낮추어야 합니다.士堂의 말씀입니다.官籍官日報籍官下書籍官 助寅官 報官 報官官下題身官際官際\n"
     ]
    }
   ],
   "source": [
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=128,\n",
    "                             do_sample=True,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print()\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?', \n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?', \n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?'\n",
    "    \n",
    "]\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "399d6ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8d6740",
   "metadata": {},
   "source": [
    "## 일반 kogpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8d03e7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = 'skt/kogpt2-base-v2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ae7ed0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답): 오늘은 두근두근 맛있었던 미식회 \n",
      "다들 너무너무 맛있게 먹었어요!\n",
      "jmilt.bangnam_farmers.official - 어제 점심 먹고 바로 갔는데\n",
      "저녁먹고 왔는데 오늘 또 뭐먹을까 고민하다가\n",
      "닭다리탕을 먹으러 왔었답니당  \n",
      "사실 닭고기도 괜찮지만\n",
      "밥도 \n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):::\n",
      "이명박 전 대통령은 재임 2년이 넘었는데\n",
      "한나라당 후보로 나경원 의원, 이재오 의원이\n",
      "그 뒤를 바짝 따라붙었다.\n",
      "그것은 이명박 대통령의 재임 기간으로\n",
      "4대 개혁입법안, 4대 강 정비사업 등 국정현안이 쌓여있었기 때문이다.\n",
      "그래서 이 대통령과 한나라당 모두는\n",
      "어려운 시기에 두 후보가\n",
      "서로에게 힘을 실어주고 있다는 것을 알고 있었다.\n",
      "이제 두 분이\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### Response(응답): \n",
      "#daily#instagood#일상#소통#사진#데일리#좋아요\n",
      "#인친#친스타그램#선팔맞팔#follow#like#mysunshin#fff#wonderfolk#yumsan#safety#photo#sonya1#todays#catson#k\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답): (당분간은\n",
      "이번 주말이 기다려져! )\n",
      "#selfie(알아보기): <17.07.10.Sat> \n",
      "저번에도 봤는데 너무 기분 좋은거 같애서 한주 잘 보냈는데 오늘 또 뵈었네요!\n",
      "그동안 힘들었던 연말모임 한 번 제대로 못가서 못갔는데, 이제 다시 올 수 있는 기회니까 너무 좋으네요\n",
      "#먹스타\n"
     ]
    }
   ],
   "source": [
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = model.generate(input_ids, max_length=max_length, num_beams=7, no_repeat_ngram_size=2,\n",
    "                             do_sample=True, temperature=2.0, top_k=50)\n",
    "    output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print()\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?', \n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?', \n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?'\n",
    "    \n",
    "]\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c353bda",
   "metadata": {},
   "source": [
    "QA에 대해 일반 모델은 제대로 된 답변을 생성하지 못하고 instruction에 대한 이해도가 떨어지는 모습을 보인다. 훈련을 한 모델의 경우, 기존 모델보다 질문을 인식하고 그에 대한 답변을 준다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
